# -*- coding: utf-8 -*-
"""ModelComparision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bgb-7dbmAc_7nGb_RAapHAN-sF9PpMXI

<a href="https://colab.research.google.com/github/Koupendra/Classification-of-Faults-in-Photovoltaic-System-using-Machine-Learning/blob/main/ModelComparision.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

#Importing necessary Libraries

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.svm import SVC # SVM
from sklearn.tree import DecisionTreeClassifier # Decision Tree
from sklearn.neighbors import KNeighborsClassifier # KNN
from sklearn.ensemble import RandomForestClassifier #Random Forest

from sklearn.metrics import accuracy_score

import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore")

# Directory for Dataset
filename = input('Dataset file location: ')
ds = pd.read_csv(filename)

ds.head(15)

ds.tail(15)

predict_result={1:"Normal",
                2:"Open Circuit",
                3:"Short Circuit"}

ds.shape

#Assigning the features and target Values
X = ds.values[:, :6]
Y = ds.values[:, 6]

print(X.shape)

def show_plot(N_ser, N_par, Irradiance):
  Filter = ds[(ds['N_Ser']==N_ser) & (ds['N_Par']==N_par) & (ds['Irradiation']==Irradiance)]
  fig = plt.figure(figsize=[8,8])
  ax = fig.add_subplot(111, projection='3d', title=f"Irradiance at {Irradiance} in {N_ser}x{N_par}",
                      xlabel="Impp (A)", ylabel="Vmpp (V)", zlabel="Temperature")

  for s in Filter['Label'].unique():
      ax.scatter(Filter.Current[Filter['Label']==s],
                Filter.Voltage[Filter['Label']==s],
                Filter.Temperature[Filter['Label']==s], label=predict_result[s])

  ax.legend()

show_plot(int(input("N_ser: ")), int(input("N_par: ")), int(input("Irradiance: ")))

# Spliting the dataset into train and test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 100)

X_train.shape

X_test.shape

train_accuracies, test_accuracies = [] ,[]

def train_the_model(model, x_train, y_train, x_test, y_test):
  model.fit(X_train,Y_train)
  train_predictions = model.predict(X_train)
  test_predictions = model.predict(X_test)
  train_accuracies.append(accuracy_score(Y_train, train_predictions)*100)
  test_accuracies.append(accuracy_score(Y_test, test_predictions)*100)
  print ("Training Accuracy : ", train_accuracies[-1])
  print ("Testing Accuracy : ", test_accuracies[-1])

# SVM Result

train_the_model(SVC(), X_train, Y_train, X_test, Y_test)

# SVM Result after hyper-tuning

params = {'kernel': ['rbf', 'poly', 'sigmoid']}
grid_search_cv = GridSearchCV(SVC(random_state=100), params)
grid_search_cv.fit(X_train, Y_train)
svm = grid_search_cv.best_estimator_
train_the_model(svm, X_train, Y_train, X_test, Y_test)

# KNN Result

train_the_model(KNeighborsClassifier(), X_train, Y_train, X_test, Y_test)

# KNN Result after hyper-tuning

params = {'n_neighbors': list(range(7, 16)),
          'weights': ['uniform', 'distance'],
          'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']}
grid_search_cv = GridSearchCV(KNeighborsClassifier(p=1), params)
grid_search_cv.fit(X_train, Y_train)
knn = grid_search_cv.best_estimator_
train_the_model(knn, X_train, Y_train, X_test, Y_test)

# Decision Tree Result

train_the_model(DecisionTreeClassifier(), X_train, Y_train, X_test, Y_test)

# Decision Tree Result after hyper-tuning

params = {'criterion':['gini', 'entropy'],
          'splitter':['best', 'random'],
          'max_features':['auto', 'sqrt', 'log2']}
grid_search_cv = GridSearchCV(DecisionTreeClassifier(), params)
grid_search_cv.fit(X_train, Y_train)
decision_tree = grid_search_cv.best_estimator_
train_the_model(decision_tree, X_train, Y_train, X_test, Y_test)

# Random Forest Result

train_the_model(RandomForestClassifier(), X_train, Y_train, X_test, Y_test)

# Random Forest Result after hyper-tuning

params = {'criterion': ['gini', 'entropy'],
          'max_features':['auto', 'sqrt', 'log2']}
grid_search_cv = GridSearchCV(RandomForestClassifier(), params)
grid_search_cv.fit(X_train, Y_train)
random_forest = grid_search_cv.best_estimator_
train_the_model(random_forest, X_train, Y_train, X_test, Y_test)

# Accuracy Comparision

labels = ["SVM", "SVM Tuned", "KNN", "KNN Tuned",
          "Decision Tree", "Desision Tree Tuned",
          "Random Forest", "Random Forest Tuned"]
x = np.arange(len(labels))
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, train_accuracies, width, label='Train')
rects2 = ax.bar(x + width/2, test_accuracies, width, label='Test')

ax.set_ylabel('Accuracy')
ax.set_title('Accuracies of Models')
ax.set_xticks(x)

ax.set_xticklabels(labels, rotation=90)
ax.legend()
ax.set_ylim()
plt.show()

3
# Testing with custom value
Nser = int(input("Nser: "))
Npar = int(input("Npar: "))
irr = float(input("Irradiance: "))
temp = float(input("Temperature: "))
voltage = float(input("Voltage: "))
current = float(input("Current: "))
test_ip = [[Nser, Npar, irr, temp, voltage, current]]

# SVM Prediction
test_pred = svm.predict(test_ip)
print(predict_result[test_pred[0]])

# KNN Prediction
test_pred = knn.predict(test_ip)
print(predict_result[test_pred[0]])

# Decision Tree Prediction
test_pred = decision_tree.predict(test_ip)
print(predict_result[test_pred[0]])

# Random Forest Prediction
test_pred = random_forest.predict(test_ip)
print(predict_result[test_pred[0]])